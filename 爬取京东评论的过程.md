# 爬取京东评论的过程

## 1.准备阶段

目标为了爬取京东评论，首先需要找到评论对应的url。首先点击评论区，F12运行检查，试图从网络中找到评论区对应的url。依据以前一些简单的实践，我的第一想法认为这可能对应一个ajax请求，于是我试图从XHR选项中寻找，但是其中并没有内容，这时我意识到json选项中也许有答案，于是打开JS标签，在切换页面的过程中我留意到到了名为productPageComments....的选项，如图1所示。点开发现其对应的相应即为json格式的评论区具体内容。通过分析url和其负载大致可知，其参数中较为重要的内容包括page和score，在选择不同评分的过程中发现，score对应的即为好评中评差评等，准备工作完毕，准备爬取数据。

## 2.爬取过程

首先进行常规导库，UA伪装，requests.get 操作。

通过将json格式化方便阅读后，依据其较强的重复性，这里我选择了使用re正则进行数据的解析：

```python
obj = re.compile(r'"id":.*?"content":"(?P<content>.*?)"'                 r'.*?"creationTime":"(?P<time>.*?)".*?"nickname":"(?P<name>.*?)"',re.S)
```

其中content, time, name即为需要爬取的评论，评论时间以及用户名

将页面数据解析的结果存到数据容器中，利用循环依次去除对应数据即可。

```python
result = obj.finditer(response.text)
    for it in result:
        r1 = it.group('name')
        r2 = it.group('content')
        r3 = it.group('time')
```

对于进行多页数据爬取的需求，需要将前面提到得page参数设置为可变，通过循环即可达到请求多页数据。

#### 关于数据的持久化存储：

在第一次尝试的时候我选择将数据存到本地txt表格中，代码如下

```python
    with open("D:/JD.txt", "a",encoding='utf-8') as f:
        for it in result:     
            f.write('评价：' + it.group('content').strip('\n'))
            f.write('时间：' + it.group('time'))
            f.write('用户名：'+ it.group('name'))
            f.write('\n')
```

在学习了MySQL的相关知识后，我尝试将数据存入数据库中。

在原先代码前定义写入数据库的函数，流程主要分为以下几步：1.连接数据库 2.设置游标 3.编写插入语句 4.执行插入语句并进行提交确认。
我选择了Navicat Premium 15方便数据库的可视化，在其中创建名为spider的库，并在此库中创建表try1进行数据的存储。

```python
def write_to_DB(name, content, time):
    db = pymysql.connect(host='localhost', port=3306, user='root', passwd='tianyi666', db='spider', charset='utf8')
    cursor = db.cursor()

    sql = """insert into try1 values ("%s", "%s", "%s")
    """ % (name, content, time)

    try:
        cursor.execute(sql)
        db.commit()
    except:
        db.rollback()

    db.close()
```

*全部代码如下：*

```python
import pymysql
import re
import requests
import json
def write_to_DB(name, content, time):
    db = pymysql.connect(host='localhost', port=3306, user='root', passwd='tianyi666', db='spider', charset='utf8')
    cursor = db.cursor()

    sql = """insert into try1 values ("%s", "%s", "%s")
    """ % (name, content, time)

    try:
        cursor.execute(sql)
        db.commit()
    except:
        db.rollback()

    db.close()
def getData(page):
    #分析参数可知 一页有十条数据 故需爬取至少五十页
    url = 'https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98&productId=100019125569&score=0&sortType=5&page={}&pageSize=10&isShadowSku=0&rid=0&fold=1'.format(page)

    #UA伪装
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36 SLBrowser/7.0.0.12151 SLBChan/103",
    }
    response = requests.get(url = url, headers=headers)
    #正则匹配
    obj = re.compile(r'"id":.*?"content":"(?P<content>.*?)"'
                     r'.*?"creationTime":"(?P<time>.*?)".*?"nickname":"(?P<name>.*?)"',re.S)
    result = obj.finditer(response.text)
    for it in result:
        r1 = it.group('name')
        r2 = it.group('content')
        r3 = it.group('time')
        write_to_DB(r1,r2,r3)
    print("ok")

if __name__ == "__main__":
    # 爬第一页和第二页
    for i in range(0,51):
        getData(i)
    print("Mission Success!")
```

GitHub链接：[链接]([CTY-icoding/Spider (github.com)](https://github.com/CTY-icoding/Spider))

